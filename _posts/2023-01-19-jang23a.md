---
title: Can Large Language Models Truly Understand Prompts? A Case Study with Negated
  Prompts
abstract: Previous work has shown that there exists a scaling law between the size
  of Language Models (LMs) and their zero-shot performance on different downstream
  NLP tasks. In this work, we show that this phenomenon does not hold when evaluating
  large LMs on tasks with \textit{negated} prompts, but instead shows an \textit{inverse}
  scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained
  LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to
  generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples,
  and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse
  on negated prompts as they scale and show a huge performance gap between the human
  performance when comparing the average score on both original and negated prompts.
  By highlighting a critical limitation of existing LMs and methods, we urge the community
  to develop new approaches of developing LMs that actually follow the given instructions.
  We provide the code and the datasets to explore negated prompts at https://github.com/joeljang/negated-prompts-for-llms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jang23a
month: 0
tex_title: Can Large Language Models Truly Understand Prompts? A Case Study with Negated
  Prompts
firstpage: 52
lastpage: 62
page: 52-62
order: 52
cycles: false
bibtex_author: Jang, Joel and Ye, Seonghyeon and Seo, Minjoon
author:
- given: Joel
  family: Jang
- given: Seonghyeon
  family: Ye
- given: Minjoon
  family: Seo
date: 2023-01-19
address:
container-title: Proceedings of The 1st Transfer Learning for Natural Language Processing
  Workshop
volume: '203'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 1
  - 19
pdf: https://proceedings.mlr.press/v203/jang23a/jang23a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
